{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pGVWhWFnaLaq",
        "outputId": "ec77a008-91fe-4e21-dbd2-5047639bff1b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision.models import vgg19\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms"
      ],
      "metadata": {
        "id": "kv7EeX3UaaVB"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3jCpYe5GaXoE",
        "outputId": "a2835790-f095-4ca2-872c-d207b77410c6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cfg = {\n",
        "    \"data\": {\n",
        "        \"content_image_path\": \"/content/drive/MyDrive/Colab Notebooks/data/content.jpg\",\n",
        "        \"style_image_path\": \"/content/drive/MyDrive/Colab Notebooks/data/style.jpg\",\n",
        "    },\n",
        "    \"trainer\": {\n",
        "        \"optimizer\": \"LBGFS\", # \"Adam\"\n",
        "        \"lr\" : 0.1,\n",
        "        \"epochs\": 100, # 1000\n",
        "        \"step_size\": 10,\n",
        "        \"alpha\": 1.0,\n",
        "        \"beta\": 1e6\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "abcrt36La7aJ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_image(image: Image.Image):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((512, 512)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    image_tensor = transform(image)\n",
        "    image_tensor = image_tensor.unsqueeze(0)\n",
        "\n",
        "    return image_tensor\n",
        "\n",
        "def postprocess_image(tensor: torch.Tensor):\n",
        "    image = tensor.to(\"cpu\").detach().numpy() # (1, c, h, w)\n",
        "    image = image.squeeze() # (c,h,w)\n",
        "    image = image.transpose(1,2,0) # (h, w, c)\n",
        "    image = image * [0.229, 0.224, 0.225] + [0.485, 0.456, 0.406] # denormalization\n",
        "    image = image.clip(0,1)*255 # 픽셀 값을 0에서 1 사이로 제한한 후 255를 곱하여 0-255 범위로 변환\n",
        "    image = image.astype(np.uint8)\n",
        "\n",
        "    return Image.fromarray(image)"
      ],
      "metadata": {
        "id": "2UHourkNaoER"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DataModule:\n",
        "    def __init__(self, content_image_path, style_image_path):\n",
        "        self.content_image_path = content_image_path\n",
        "        self.style_image_path = style_image_path\n",
        "\n",
        "    def _load_preprocessed_image(self, image_path):\n",
        "        image = Image.open(image_path)\n",
        "        image_tensor = preprocess_image(image)\n",
        "\n",
        "        return image_tensor\n",
        "\n",
        "    def get_image_tensors(self):\n",
        "        content_image_tensor = self._load_preprocessed_image(self.content_image_path)\n",
        "        style_image_tensor = self._load_preprocessed_image(self.style_image_path)\n",
        "\n",
        "        return content_image_tensor, style_image_tensor\n",
        "\n",
        "    def get_noise_image_tensor_to_image(self, noise_image_tensor):\n",
        "        return postprocess_image(noise_image_tensor)"
      ],
      "metadata": {
        "id": "omS0kT80auAx"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Data\n",
        "datamodule = DataModule(cfg[\"data\"][\"content_image_path\"], cfg[\"data\"][\"style_image_path\"])\n",
        "content_image_tensor, style_image_tensor = datamodule.get_image_tensors()\n",
        "content_image_tensor = content_image_tensor.to(device)\n",
        "style_image_tensor = style_image_tensor.to(device)"
      ],
      "metadata": {
        "id": "g6MyavBFa0Ep"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conv = {\n",
        "    \"conv1_1\" : 0,\n",
        "    \"conv2_1\" : 5,\n",
        "    \"conv3_1\" : 10,\n",
        "    \"conv4_1\" : 19,\n",
        "    \"conv5_1\" : 28,\n",
        "    \"conv4_2\" : 21\n",
        "}\n",
        "\n",
        "class StyleTransfer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(StyleTransfer, self).__init__()\n",
        "        self.vgg19 = vgg19(pretrained=True)\n",
        "        self.vgg19_features = self.vgg19.features\n",
        "\n",
        "        self.style_layer = [conv[\"conv1_1\"], conv[\"conv2_1\"], conv[\"conv3_1\"], conv[\"conv4_1\"], conv[\"conv5_1\"]]\n",
        "        self.content_layer = conv[\"conv4_2\"]\n",
        "\n",
        "    def forward(self, x, mode: str):\n",
        "        features = []\n",
        "\n",
        "        if mode == \"style\":\n",
        "            for i in range(len(self.vgg19_features)):\n",
        "                x = self.vgg19_features[i](x)\n",
        "                if i in self.style_layer:\n",
        "                    features.append(x)\n",
        "        elif mode == \"content\":\n",
        "            for i in range(len(self.vgg19_features)):\n",
        "                x = self.vgg19_features[i](x)\n",
        "                if i == self.content_layer:\n",
        "                    features.append(x)\n",
        "        else:\n",
        "            raise ValueError(\"Invalid mode\")\n",
        "\n",
        "        return features"
      ],
      "metadata": {
        "id": "G_HBAZiGdHCz"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Model\n",
        "model = StyleTransfer().to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EZYKZOHYdJQR",
        "outputId": "4323c24c-11e9-4abe-a113-4b2efc141596"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n",
            "100%|██████████| 548M/548M [00:03<00:00, 164MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ContentLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ContentLoss, self).__init__()\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        loss = F.mse_loss(input, target)\n",
        "        return loss\n",
        "\n",
        "class StyleLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(StyleLoss, self).__init__()\n",
        "\n",
        "    def gram_matrix(self, input):\n",
        "        b, c, h, w = input.size()\n",
        "        features = input.view(b, c, -1)\n",
        "        gram = torch.bmm(features, features.transpose(1, 2))\n",
        "        return gram.div(b * c * h * w) # normalize\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        loss = F.mse_loss(self.gram_matrix(input), self.gram_matrix(target))\n",
        "        return loss"
      ],
      "metadata": {
        "id": "lLYXQZUAdLFB"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Loss\n",
        "content_loss_fn = ContentLoss()\n",
        "style_loss_fn = StyleLoss()"
      ],
      "metadata": {
        "id": "etRzKwG8dSBk"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Noise Image\n",
        "noise_image_tensor = content_image_tensor.clone().requires_grad_(True)"
      ],
      "metadata": {
        "id": "UMntTmCedU-Q"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if cfg[\"trainer\"][\"optimizer\"] == \"Adam\":\n",
        "    optimizer = optim.Adam([noise_image_tensor], lr=cfg[\"trainer\"][\"lr\"])\n",
        "elif cfg[\"trainer\"][\"optimizer\"] == \"LBGFS\":\n",
        "    optimizer = optim.LBFGS([noise_image_tensor], lr=cfg[\"trainer\"][\"lr\"])\n",
        "else:\n",
        "    raise ValueError(f\"Select optimizer from Adam and LBGFS\")"
      ],
      "metadata": {
        "id": "ehOcomhDdZmY"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_loss(\n",
        "    model,\n",
        "    content_image_tensor,\n",
        "    style_image_tensor,\n",
        "    noise_image_tensor,\n",
        "    content_loss_fn,\n",
        "    style_loss_fn,\n",
        "    cfg,\n",
        "):\n",
        "    x_content_list = model(noise_image_tensor, mode=\"content\")\n",
        "    y_content_list = model(content_image_tensor, mode=\"content\")\n",
        "\n",
        "    x_style_list = model(noise_image_tensor, mode=\"style\")\n",
        "    y_style_list = model(style_image_tensor, mode=\"style\")\n",
        "\n",
        "    content_loss, style_loss, total_loss = 0.0, 0.0, 0.0\n",
        "\n",
        "    for x_content, y_content in zip(x_content_list, y_content_list):\n",
        "        content_loss += content_loss_fn(x_content, y_content)\n",
        "\n",
        "    for x_style, y_style in zip(x_style_list, y_style_list):\n",
        "        style_loss += style_loss_fn(x_style, y_style)\n",
        "\n",
        "    total_loss = cfg[\"trainer\"][\"alpha\"] * content_loss + cfg[\"trainer\"][\"beta\"] * style_loss\n",
        "\n",
        "    return content_loss, style_loss, total_loss"
      ],
      "metadata": {
        "id": "zhbHHT4gd875"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(\n",
        "    model,\n",
        "    datamodule,\n",
        "    content_image_tensor,\n",
        "    style_image_tensor,\n",
        "    noise_image_tensor,\n",
        "    optimizer,\n",
        "    content_loss_fn,\n",
        "    style_loss_fn,\n",
        "    current_epoch,\n",
        "    cfg,\n",
        "):\n",
        "    model.eval()\n",
        "\n",
        "    def closure():\n",
        "        optimizer.zero_grad()\n",
        "        content_loss, style_loss, total_loss = compute_loss(\n",
        "            model,\n",
        "            content_image_tensor,\n",
        "            style_image_tensor,\n",
        "            noise_image_tensor,\n",
        "            content_loss_fn,\n",
        "            style_loss_fn,\n",
        "            cfg,\n",
        "        )\n",
        "        total_loss.backward()\n",
        "        return total_loss\n",
        "\n",
        "    if isinstance(optimizer, optim.LBFGS):\n",
        "        optimizer.step(closure)\n",
        "    else:\n",
        "        content_loss, style_loss, total_loss = compute_loss(\n",
        "            model,\n",
        "            content_image_tensor,\n",
        "            style_image_tensor,\n",
        "            noise_image_tensor,\n",
        "            content_loss_fn,\n",
        "            style_loss_fn,\n",
        "            cfg,\n",
        "        )\n",
        "        optimizer.zero_grad()\n",
        "        total_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    if current_epoch % cfg[\"trainer\"][\"step_size\"] == 0:\n",
        "        if isinstance(optimizer, optim.LBFGS):\n",
        "            with torch.no_grad():\n",
        "                content_loss, style_loss, total_loss = compute_loss(\n",
        "                    model,\n",
        "                    content_image_tensor,\n",
        "                    style_image_tensor,\n",
        "                    noise_image_tensor,\n",
        "                    content_loss_fn,\n",
        "                    style_loss_fn,\n",
        "                    cfg,\n",
        "                )\n",
        "                print(\n",
        "                    f\"Content Loss: {content_loss.cpu().item()}, Style Loss: {style_loss.cpu().item()}, Total Loss: {total_loss.cpu().item()}\"\n",
        "                )\n",
        "        else:\n",
        "            print(\n",
        "                f\"Content Loss: {content_loss.cpu().item()}, Style Loss: {style_loss.cpu().item()}, Total Loss: {total_loss.cpu().item()}\"\n",
        "            )\n",
        "\n",
        "        gen_image = datamodule.get_noise_image_tensor_to_image(noise_image_tensor)\n",
        "        if not os.path.exists(r\"/content/drive/MyDrive/Colab Notebooks/output\"):\n",
        "            os.makedirs(r\"/content/drive/MyDrive/Colab Notebooks/output\", exist_ok=True)\n",
        "        gen_image.save(\n",
        "            f\"/content/drive/MyDrive/Colab Notebooks/output/{cfg['trainer']['optimizer']}_epoch_{current_epoch}.jpg\"\n",
        "        )"
      ],
      "metadata": {
        "id": "2rHGyyYKd0uA"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in tqdm(range(cfg[\"trainer\"][\"epochs\"])):\n",
        "    train(\n",
        "        model,\n",
        "        datamodule,\n",
        "        content_image_tensor,\n",
        "        style_image_tensor,\n",
        "        noise_image_tensor,\n",
        "        optimizer,\n",
        "        content_loss_fn,\n",
        "        style_loss_fn,\n",
        "        current_epoch=epoch,\n",
        "        cfg=cfg,\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Az8V3wBCdYQC",
        "outputId": "6718354e-e495-44f3-90cf-710731ca79fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|          | 1/100 [00:09<15:17,  9.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Content Loss: 4.194981575012207, Style Loss: 0.0002535551320761442, Total Loss: 257.7501220703125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  3%|▎         | 3/100 [00:25<13:14,  8.19s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "seQzmKkvfL16"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}